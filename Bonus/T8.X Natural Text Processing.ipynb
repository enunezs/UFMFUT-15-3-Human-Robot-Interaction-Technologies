{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup"
      ],
      "metadata": {
        "id": "Tb5YpOmpk-4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy contractions numpy SpeechRecognition gtts pygame openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd-et3eykBtt",
        "outputId": "5cf3d086-62db-4ab9-a74e-d97d23dd73ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, SpeechRecognition, gtts, contractions\n",
            "Successfully installed SpeechRecognition-3.11.0 anyascii-0.3.2 contractions-0.1.73 gtts-2.5.4 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download nltk additional lists"
      ],
      "metadata": {
        "id": "3m3VZKKGkm5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader popular\n",
        "!python -m nltk.downloader punkt_tab\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader averaged_perceptron_tagger_eng\n",
        "!python -m nltk.downloader maxent_ne_chunker_tab\n",
        "!python -m nltk.downloader vader_lexicon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2KGRMWkkkjy",
        "outputId": "989de102-8948-41be-d157-aa084f646309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Spacy lists"
      ],
      "metadata": {
        "id": "y2LFV_d9ku3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1W3_6tckt5q",
        "outputId": "f3346a29-73df-4caf-ef1b-706f0d43b4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Natural Text Processing"
      ],
      "metadata": {
        "id": "kvR1UJ0Mk6Q9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDD_mX9vixxV",
        "outputId": "882dbaca-3233-4575-e881-7e10f52426c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for processing\n",
        "text = \"\"\"And so I cry sometimes when I'm lying in bed\n",
        "Just to get it all out what's in my head\n",
        "And I, I am feeling a little peculiar\n",
        "And so I wake in the morning and I step outside\n",
        "And I take a deep breath and I get real high\n",
        "And I scream from the top of my lungs\n",
        "'What's going on?'\"\"\""
      ],
      "metadata": {
        "id": "3vONiO0njR1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Normalization"
      ],
      "metadata": {
        "id": "nemZ4yMeFed0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenization"
      ],
      "metadata": {
        "id": "VQNW7nEgEtos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets compare the tokenization in the two popular librariers: NLTK and Spacy\n",
        "\n"
      ],
      "metadata": {
        "id": "rAo064Ukjl2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK Text Normalization, Tokenization, and POS Tagging\n",
        "def nltk_processing(text):\n",
        "    # Normalization (lowercase)\n",
        "    normalized_text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(normalized_text)\n",
        "\n",
        "    # POS Tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    return tokens, pos_tags\n",
        "\n",
        "# Run NLTK processing\n",
        "nltk_tokens, nltk_pos_tags = nltk_processing(text)\n",
        "print(\"NLTK Tokenization:\")\n",
        "print(nltk_tokens)\n",
        "print(\"NLTK POS Tags:\")\n",
        "print(nltk_pos_tags)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx41GuNhjJEj",
        "outputId": "3d963f76-e8f3-45d3-a97d-89b0e342ea46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokenization:\n",
            "['and', 'so', 'i', 'cry', 'sometimes', 'when', 'i', \"'m\", 'lying', 'in', 'bed', 'just', 'to', 'get', 'it', 'all', 'out', 'what', \"'s\", 'in', 'my', 'head', 'and', 'i', ',', 'i', 'am', 'feeling', 'a', 'little', 'peculiar', 'and', 'so', 'i', 'wake', 'in', 'the', 'morning', 'and', 'i', 'step', 'outside', 'and', 'i', 'take', 'a', 'deep', 'breath', 'and', 'i', 'get', 'real', 'high', 'and', 'i', 'scream', 'from', 'the', 'top', 'of', 'my', 'lungs', \"'what\", \"'s\", 'going', 'on', '?', \"'\"]\n",
            "NLTK POS Tags:\n",
            "[('and', 'CC'), ('so', 'RB'), ('i', 'JJ'), ('cry', 'NN'), ('sometimes', 'RB'), ('when', 'WRB'), ('i', 'JJ'), (\"'m\", 'VBP'), ('lying', 'VBG'), ('in', 'IN'), ('bed', 'NN'), ('just', 'RB'), ('to', 'TO'), ('get', 'VB'), ('it', 'PRP'), ('all', 'DT'), ('out', 'RP'), ('what', 'WP'), (\"'s\", 'VBZ'), ('in', 'IN'), ('my', 'PRP$'), ('head', 'NN'), ('and', 'CC'), ('i', 'NN'), (',', ','), ('i', 'NN'), ('am', 'VBP'), ('feeling', 'VBG'), ('a', 'DT'), ('little', 'JJ'), ('peculiar', 'NN'), ('and', 'CC'), ('so', 'RB'), ('i', 'JJ'), ('wake', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('and', 'CC'), ('i', 'JJ'), ('step', 'NN'), ('outside', 'IN'), ('and', 'CC'), ('i', 'JJ'), ('take', 'VBP'), ('a', 'DT'), ('deep', 'JJ'), ('breath', 'NN'), ('and', 'CC'), ('i', 'NN'), ('get', 'VBP'), ('real', 'JJ'), ('high', 'JJ'), ('and', 'CC'), ('i', 'JJ'), ('scream', 'NN'), ('from', 'IN'), ('the', 'DT'), ('top', 'NN'), ('of', 'IN'), ('my', 'PRP$'), ('lungs', 'NNS'), (\"'what\", 'WP'), (\"'s\", 'VBZ'), ('going', 'VBG'), ('on', 'IN'), ('?', '.'), (\"'\", \"''\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy Processing\n",
        "def spacy_processing(text):\n",
        "    # Load SpaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Process text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    # POS tagging\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    return tokens, pos_tags\n",
        "\n",
        "# Run SpaCy processing\n",
        "spacy_tokens, spacy_pos_tags = spacy_processing(text)\n",
        "print(\"\\nSpaCy Tokenization:\")\n",
        "print(spacy_tokens)\n",
        "print(\"SpaCy POS Tags:\")\n",
        "print(spacy_pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QskwQTrjJ6z",
        "outputId": "aa782ea8-81bc-40b4-8f2c-ae2b87ed01d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SpaCy Tokenization:\n",
            "['And', 'so', 'I', 'cry', 'sometimes', 'when', 'I', \"'m\", 'lying', 'in', 'bed', '\\n', 'Just', 'to', 'get', 'it', 'all', 'out', 'what', \"'s\", 'in', 'my', 'head', '\\n', 'And', 'I', ',', 'I', 'am', 'feeling', 'a', 'little', 'peculiar', '\\n', 'And', 'so', 'I', 'wake', 'in', 'the', 'morning', 'and', 'I', 'step', 'outside', '\\n', 'And', 'I', 'take', 'a', 'deep', 'breath', 'and', 'I', 'get', 'real', 'high', '\\n', 'And', 'I', 'scream', 'from', 'the', 'top', 'of', 'my', 'lungs', '\\n', \"'\", 'What', \"'s\", 'going', 'on', '?', \"'\"]\n",
            "SpaCy POS Tags:\n",
            "[('And', 'CCONJ'), ('so', 'ADV'), ('I', 'PRON'), ('cry', 'VERB'), ('sometimes', 'ADV'), ('when', 'SCONJ'), ('I', 'PRON'), (\"'m\", 'AUX'), ('lying', 'VERB'), ('in', 'ADP'), ('bed', 'NOUN'), ('\\n', 'SPACE'), ('Just', 'ADV'), ('to', 'PART'), ('get', 'VERB'), ('it', 'PRON'), ('all', 'PRON'), ('out', 'ADP'), ('what', 'PRON'), (\"'s\", 'AUX'), ('in', 'ADP'), ('my', 'PRON'), ('head', 'NOUN'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('I', 'PRON'), (',', 'PUNCT'), ('I', 'PRON'), ('am', 'AUX'), ('feeling', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('peculiar', 'ADJ'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('so', 'ADV'), ('I', 'PRON'), ('wake', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('morning', 'NOUN'), ('and', 'CCONJ'), ('I', 'PRON'), ('step', 'VERB'), ('outside', 'ADV'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('I', 'PRON'), ('take', 'VERB'), ('a', 'DET'), ('deep', 'ADJ'), ('breath', 'NOUN'), ('and', 'CCONJ'), ('I', 'PRON'), ('get', 'VERB'), ('real', 'ADV'), ('high', 'ADJ'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('I', 'PRON'), ('scream', 'VERB'), ('from', 'ADP'), ('the', 'DET'), ('top', 'NOUN'), ('of', 'ADP'), ('my', 'PRON'), ('lungs', 'NOUN'), ('\\n', 'SPACE'), (\"'\", 'PUNCT'), ('What', 'PRON'), (\"'s\", 'AUX'), ('going', 'VERB'), ('on', 'ADP'), ('?', 'PUNCT'), (\"'\", 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "U4rvb6bCHNut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from: https://shariqhameed127.medium.com/text-normalization-nlp-basics-part-4-of-10-a2d15c3754dc\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = text\n",
        "#sentence = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for word in words:\n",
        "    print(word, \": \", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-iu-4LeHOhV",
        "outputId": "b0a18df1-644b-4db9-9be5-be2a129ffaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And :  and\n",
            "so :  so\n",
            "I :  i\n",
            "cry :  cri\n",
            "sometimes :  sometim\n",
            "when :  when\n",
            "I :  i\n",
            "'m :  'm\n",
            "lying :  lie\n",
            "in :  in\n",
            "bed :  bed\n",
            "Just :  just\n",
            "to :  to\n",
            "get :  get\n",
            "it :  it\n",
            "all :  all\n",
            "out :  out\n",
            "what :  what\n",
            "'s :  's\n",
            "in :  in\n",
            "my :  my\n",
            "head :  head\n",
            "And :  and\n",
            "I :  i\n",
            ", :  ,\n",
            "I :  i\n",
            "am :  am\n",
            "feeling :  feel\n",
            "a :  a\n",
            "little :  littl\n",
            "peculiar :  peculiar\n",
            "And :  and\n",
            "so :  so\n",
            "I :  i\n",
            "wake :  wake\n",
            "in :  in\n",
            "the :  the\n",
            "morning :  morn\n",
            "and :  and\n",
            "I :  i\n",
            "step :  step\n",
            "outside :  outsid\n",
            "And :  and\n",
            "I :  i\n",
            "take :  take\n",
            "a :  a\n",
            "deep :  deep\n",
            "breath :  breath\n",
            "and :  and\n",
            "I :  i\n",
            "get :  get\n",
            "real :  real\n",
            "high :  high\n",
            "And :  and\n",
            "I :  i\n",
            "scream :  scream\n",
            "from :  from\n",
            "the :  the\n",
            "top :  top\n",
            "of :  of\n",
            "my :  my\n",
            "lungs :  lung\n",
            "'What :  'what\n",
            "'s :  's\n",
            "going :  go\n",
            "on :  on\n",
            "? :  ?\n",
            "' :  '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "w-I_uOjlHGK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "# Load the SpaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample words for lemmatization\n",
        "#words = [\"foxes\", \"jumping\", \"rocks\", \"corpora\", \"better\"]\n",
        "words = words # from before!\n",
        "\n",
        "# Initialize the NLTK WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatization using NLTK\n",
        "print(\"\\nLemmatization with NLTK:\")\n",
        "for word in words:\n",
        "    # Specify part of speech for better lemmatization if necessary\n",
        "    pos = \"n\" if word in [\"rocks\", \"corpora\"] else \"v\" if word == \"jumping\" else \"a\" if word == \"better\" else None\n",
        "    lemmatized_nltk = lemmatizer.lemmatize(word, pos=pos) if pos else lemmatizer.lemmatize(word)\n",
        "    print(f\"{word}: {lemmatized_nltk}\")\n",
        "\n",
        "# Lemmatization using SpaCy\n",
        "print(\"Lemmatization with SpaCy:\")\n",
        "for word in words:\n",
        "    doc = nlp(word)\n",
        "    lemmatized_spacy = [token.lemma_ for token in doc]\n",
        "    print(f\"{word}: {lemmatized_spacy[0]}\")  # Output the lemmatized form\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shkc35XhHFLM",
        "outputId": "5e1e400d-d8b7-4f58-e32c-e6f033b10ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization with NLTK:\n",
            "And: And\n",
            "so: so\n",
            "I: I\n",
            "cry: cry\n",
            "sometimes: sometimes\n",
            "when: when\n",
            "I: I\n",
            "'m: 'm\n",
            "lying: lying\n",
            "in: in\n",
            "bed: bed\n",
            "Just: Just\n",
            "to: to\n",
            "get: get\n",
            "it: it\n",
            "all: all\n",
            "out: out\n",
            "what: what\n",
            "'s: 's\n",
            "in: in\n",
            "my: my\n",
            "head: head\n",
            "And: And\n",
            "I: I\n",
            ",: ,\n",
            "I: I\n",
            "am: am\n",
            "feeling: feeling\n",
            "a: a\n",
            "little: little\n",
            "peculiar: peculiar\n",
            "And: And\n",
            "so: so\n",
            "I: I\n",
            "wake: wake\n",
            "in: in\n",
            "the: the\n",
            "morning: morning\n",
            "and: and\n",
            "I: I\n",
            "step: step\n",
            "outside: outside\n",
            "And: And\n",
            "I: I\n",
            "take: take\n",
            "a: a\n",
            "deep: deep\n",
            "breath: breath\n",
            "and: and\n",
            "I: I\n",
            "get: get\n",
            "real: real\n",
            "high: high\n",
            "And: And\n",
            "I: I\n",
            "scream: scream\n",
            "from: from\n",
            "the: the\n",
            "top: top\n",
            "of: of\n",
            "my: my\n",
            "lungs: lung\n",
            "'What: 'What\n",
            "'s: 's\n",
            "going: going\n",
            "on: on\n",
            "?: ?\n",
            "': '\n",
            "Lemmatization with SpaCy:\n",
            "And: and\n",
            "so: so\n",
            "I: I\n",
            "cry: cry\n",
            "sometimes: sometimes\n",
            "when: when\n",
            "I: I\n",
            "'m: '\n",
            "lying: lie\n",
            "in: in\n",
            "bed: bed\n",
            "Just: just\n",
            "to: to\n",
            "get: get\n",
            "it: it\n",
            "all: all\n",
            "out: out\n",
            "what: what\n",
            "'s: be\n",
            "in: in\n",
            "my: my\n",
            "head: head\n",
            "And: and\n",
            "I: I\n",
            ",: ,\n",
            "I: I\n",
            "am: be\n",
            "feeling: feel\n",
            "a: a\n",
            "little: little\n",
            "peculiar: peculiar\n",
            "And: and\n",
            "so: so\n",
            "I: I\n",
            "wake: wake\n",
            "in: in\n",
            "the: the\n",
            "morning: morning\n",
            "and: and\n",
            "I: I\n",
            "step: step\n",
            "outside: outside\n",
            "And: and\n",
            "I: I\n",
            "take: take\n",
            "a: a\n",
            "deep: deep\n",
            "breath: breath\n",
            "and: and\n",
            "I: I\n",
            "get: get\n",
            "real: real\n",
            "high: high\n",
            "And: and\n",
            "I: I\n",
            "scream: scream\n",
            "from: from\n",
            "the: the\n",
            "top: top\n",
            "of: of\n",
            "my: my\n",
            "lungs: lung\n",
            "'What: '\n",
            "'s: be\n",
            "going: go\n",
            "on: on\n",
            "?: ?\n",
            "': '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tagging"
      ],
      "metadata": {
        "id": "P5zd6FsAG0Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified from: https://shariqhameed127.medium.com/pos-parts-of-speech-tagging-nlp-basics-part-5-of-10-1a4a5c1b721c\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentence\n",
        "#sentence = \"Cooking is delightful\"\n",
        "sentence = text\n",
        "\n",
        "def pos_tagger_nltk(sentence):\n",
        "  words = word_tokenize(sentence)\n",
        "\n",
        "  return nltk.pos_tag(words)\n",
        "\n",
        "\n",
        "def pos_tagger_spacy(sentence):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(pos_tagger_nltk(sentence))\n",
        "print(pos_tagger_spacy(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPvi__NSG10k",
        "outputId": "9053b7c2-4fba-4874-84e0-f7215739e1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('And', 'CC'), ('so', 'RB'), ('I', 'PRP'), ('cry', 'VBP'), ('sometimes', 'RB'), ('when', 'WRB'), ('I', 'PRP'), (\"'m\", 'VBP'), ('lying', 'VBG'), ('in', 'IN'), ('bed', 'NN'), ('Just', 'NNP'), ('to', 'TO'), ('get', 'VB'), ('it', 'PRP'), ('all', 'DT'), ('out', 'RP'), ('what', 'WP'), (\"'s\", 'VBZ'), ('in', 'IN'), ('my', 'PRP$'), ('head', 'NN'), ('And', 'CC'), ('I', 'PRP'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('feeling', 'VBG'), ('a', 'DT'), ('little', 'JJ'), ('peculiar', 'NN'), ('And', 'CC'), ('so', 'RB'), ('I', 'PRP'), ('wake', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('step', 'VBP'), ('outside', 'JJ'), ('And', 'CC'), ('I', 'PRP'), ('take', 'VBP'), ('a', 'DT'), ('deep', 'JJ'), ('breath', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('get', 'VBP'), ('real', 'JJ'), ('high', 'JJ'), ('And', 'CC'), ('I', 'PRP'), ('scream', 'VBP'), ('from', 'IN'), ('the', 'DT'), ('top', 'NN'), ('of', 'IN'), ('my', 'PRP$'), ('lungs', 'NNS'), (\"'What\", 'WP'), (\"'s\", 'VBZ'), ('going', 'VBG'), ('on', 'IN'), ('?', '.'), (\"'\", \"''\")]\n",
            "[('And', 'CCONJ'), ('so', 'ADV'), ('I', 'PRON'), ('cry', 'VERB'), ('sometimes', 'ADV'), ('when', 'SCONJ'), ('I', 'PRON'), (\"'m\", 'AUX'), ('lying', 'VERB'), ('in', 'ADP'), ('bed', 'NOUN'), ('\\n', 'SPACE'), ('Just', 'ADV'), ('to', 'PART'), ('get', 'VERB'), ('it', 'PRON'), ('all', 'PRON'), ('out', 'ADP'), ('what', 'PRON'), (\"'s\", 'AUX'), ('in', 'ADP'), ('my', 'PRON'), ('head', 'NOUN'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('I', 'PRON'), (',', 'PUNCT'), ('I', 'PRON'), ('am', 'AUX'), ('feeling', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('peculiar', 'ADJ'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('so', 'ADV'), ('I', 'PRON'), ('wake', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('morning', 'NOUN'), ('and', 'CCONJ'), ('I', 'PRON'), ('step', 'VERB'), ('outside', 'ADV'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('I', 'PRON'), ('take', 'VERB'), ('a', 'DET'), ('deep', 'ADJ'), ('breath', 'NOUN'), ('and', 'CCONJ'), ('I', 'PRON'), ('get', 'VERB'), ('real', 'ADV'), ('high', 'ADJ'), ('\\n', 'SPACE'), ('And', 'CCONJ'), ('I', 'PRON'), ('scream', 'VERB'), ('from', 'ADP'), ('the', 'DET'), ('top', 'NOUN'), ('of', 'ADP'), ('my', 'PRON'), ('lungs', 'NOUN'), ('\\n', 'SPACE'), (\"'\", 'PUNCT'), ('What', 'PRON'), (\"'s\", 'AUX'), ('going', 'VERB'), ('on', 'ADP'), ('?', 'PUNCT'), (\"'\", 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "-8nQl4b2Hf70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified from: https://shariqhameed127.medium.com/named-entity-recognition-ner-nlp-basics-part-6-of-10-4cf3df102bf4\n",
        "import nltk, spacy\n",
        "\n",
        "text = \"Tesla opened a new office in Tokyo today\"\n",
        "#text = text\n",
        "\n",
        "def ner_nltk(text):\n",
        "    entities = {}\n",
        "\n",
        "    tokens = nltk.sent_tokenize(text)\n",
        "    for sent in tokens:\n",
        "        word_tokens = nltk.word_tokenize(sent)\n",
        "        for i, chunk in enumerate(nltk.ne_chunk(nltk.pos_tag(word_tokens))):\n",
        "            if hasattr(chunk, 'label'):\n",
        "                entities[word_tokens[i]] = chunk.label()\n",
        "\n",
        "\n",
        "    return entities\n",
        "\n",
        "print(\"NER with nltk\")\n",
        "print(ner_nltk(text))\n",
        "\n",
        "def ner_spacy(sentence):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "    return entities\n",
        "\n",
        "print(\"NER with spacy\")\n",
        "print(ner_spacy(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3jwnw6fHgNk",
        "outputId": "5523bbc0-fa06-4847-e8c1-aa8801596054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER with nltk\n",
            "{'Tesla': 'PERSON', 'Tokyo': 'GPE'}\n",
            "NER with spacy\n",
            "[('Tesla', 'ORG'), ('Tokyo', 'GPE'), ('today', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Sentiment Analsysis"
      ],
      "metadata": {
        "id": "bfXldCUCH4DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from: https://medium.com/@mail4sameera/sentiment-analysis-nlp-understanding-emotions-in-text-data-a9dfa39db2de\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Sample text\n",
        "# text = \"I absolutely love this product! It exceeded my expectations.\\nI am not satisfied with the service.\\nOverall, it was an okay experience.\"\n",
        "text = \"Maths\"\n",
        "\n",
        "# Initialize SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Split the text by newline characters\n",
        "lines = text.split('\\n')\n",
        "\n",
        "# Iterate over each line and analyze sentiment\n",
        "for line in lines:\n",
        "    if line.strip():  # Check if the line is not empty\n",
        "        # Get sentiment scores\n",
        "        sentiment_scores = sia.polarity_scores(line)\n",
        "\n",
        "        # Interpret sentiment scores\n",
        "        if sentiment_scores['compound'] >= 0.05:\n",
        "            sentiment = \"Positive\"\n",
        "        elif sentiment_scores['compound'] <= -0.05:\n",
        "            sentiment = \"Negative\"\n",
        "        else:\n",
        "            sentiment = \"Neutral\"\n",
        "\n",
        "        print(\"Text to analyze: {0}\\n\".format(line))\n",
        "        print(f\"The sentiment of the text is: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyUEl1q2H5xV",
        "outputId": "78b00ab4-2771-4fa4-f839-9d4b0368e153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text to analyze: Maths\n",
            "\n",
            "The sentiment of the text is: Neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does it work?"
      ],
      "metadata": {
        "id": "zojXygr7WSYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from NLTK\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "JICe3sNiWW1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876ffc71-ec0d-42dc-e3a2-983b3d966642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "9-2ZWZZGWauJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the number of instances for training and testing\n",
        "n_instances = 100\n",
        "\n",
        "# Load subjective and objective sentences from the subjectivity corpus\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "\n",
        "# Check the number of subjective and objective documents\n",
        "print(len(subj_docs), len(obj_docs))  # Expected output: (100, 100)\n",
        "\n",
        "# Example of a subjective document\n",
        "print(subj_docs[0])  # Displays a sentence from the subjective category\n",
        "\n",
        "# Split the documents into training and testing sets\n",
        "train_subj_docs = subj_docs[:80]\n",
        "test_subj_docs = subj_docs[80:100]\n",
        "train_obj_docs = obj_docs[:80]\n",
        "test_obj_docs = obj_docs[80:100]\n",
        "\n",
        "# Combine the training and testing documents\n",
        "training_docs = train_subj_docs + train_obj_docs\n",
        "testing_docs = test_subj_docs + test_obj_docs\n",
        "\n",
        "# Initialize the SentimentAnalyzer\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "\n",
        "# Handle negation and extract all words from the training data\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
        "\n",
        "# Extract unigram features (single words) from the words list\n",
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "\n",
        "# Display the number of unique unigram features\n",
        "print(len(unigram_feats))  # Expected output: 83\n",
        "\n",
        "# Add the unigram features to the sentiment analyzer\n",
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
        "\n",
        "# Apply features to the training and testing sets\n",
        "training_set = sentim_analyzer.apply_features(training_docs)\n",
        "test_set = sentim_analyzer.apply_features(testing_docs)\n",
        "\n",
        "# Train the classifier using Naive Bayes\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "for key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "    print(f'{key}: {value}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm5PmL0CUDPH",
        "outputId": "dd976e9d-f500-4bf3-8db6-22f97720f526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 100\n",
            "(['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj')\n",
            "83\n",
            "Training classifier\n",
            "Evaluating NaiveBayesClassifier results...\n",
            "Accuracy: 0.8\n",
            "F-measure [obj]: 0.8\n",
            "F-measure [subj]: 0.8\n",
            "Precision [obj]: 0.8\n",
            "Precision [subj]: 0.8\n",
            "Recall [obj]: 0.8\n",
            "Recall [subj]: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tricky cases"
      ],
      "metadata": {
        "id": "mODK4HjPW_kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Now working with VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import tokenize\n",
        "\n",
        "# Define a list of sentences to analyze with VADER\n",
        "sentences = [\n",
        "    \"VADER is smart, handsome, and funny.\",  # Positive sentence\n",
        "    \"VADER is smart, handsome, and funny!\",  # Punctuation emphasis\n",
        "    \"VADER is very smart, handsome, and funny.\",  # Booster words\n",
        "    \"VADER is VERY SMART, handsome, and FUNNY.\",  # ALL CAPS emphasis\n",
        "    \"VADER is VERY SMART, handsome, and FUNNY!!!\",  # Combination of signals\n",
        "    \"VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",  # Close to max intensity\n",
        "    \"The book was good.\",  # Positive sentence\n",
        "    \"The book was kind of good.\",  # Qualified positive\n",
        "    \"The plot was good, but the characters are uncompelling.\",  # Mixed sentiment\n",
        "    \"A really bad, horrible book.\",  # Negative sentence\n",
        "    \":) and :D\",  # Emoticons\n",
        "    \"\",  # Empty string\n",
        "    \"Today sux\",  # Negative slang\n",
        "    \"Today sux!\",  # Negative slang with emphasis\n",
        "    \"Today SUX!\",  # Negative slang with all caps\n",
        "    \"Today kinda sux! But I'll get by, lol\"  # Mixed sentiment with slang\n",
        "]\n",
        "\n",
        "# Example paragraph to analyze\n",
        "paragraph = \"\"\"\n",
        "It was one of the worst movies I've seen, despite good reviews.\n",
        "Unbelievably bad acting!! Poor direction. VERY poor production.\n",
        "The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the paragraph into sentences\n",
        "lines_list = tokenize.sent_tokenize(paragraph)\n",
        "\n",
        "# Add these sentences to the list for analysis\n",
        "sentences.extend(lines_list)\n",
        "\n",
        "# Analyze tricky sentences with mixed sentiment\n",
        "tricky_sentences = [\n",
        "    \"Most automated sentiment analysis tools are shit.\",\n",
        "    \"VADER sentiment analysis is the shit.\",\n",
        "    \"Sentiment analysis has never been good.\",\n",
        "    \"Sentiment analysis with VADER has never been this good.\",\n",
        "    \"Warren Beatty has never been so entertaining.\",\n",
        "    \"I won't say that the movie is astounding and I wouldn't claim that the movie is too banal either.\",\n",
        "    \"I like to hate Michael Bay films, but I couldn't fault this one.\",\n",
        "    \"I like to hate Michael Bay films, BUT I couldn't help but fault this one.\",\n",
        "    \"It's one thing to watch an Uwe Boll film, but another thing entirely to pay for it.\",\n",
        "    \"The movie was too good.\",\n",
        "    \"This movie was actually neither that funny, nor super witty.\",\n",
        "    \"This movie doesn't care about cleverness, wit or any other kind of intelligent humor.\",\n",
        "    \"Those who find ugly meanings in beautiful things are corrupt without being charming.\",\n",
        "    \"There are slow and repetitive parts, BUT it has just enough spice to keep it interesting.\",\n",
        "    \"The script is not fantastic, but the acting is decent and the cinematography is EXCELLENT!\",\n",
        "    \"Roger Dodger is one of the most compelling variations on this theme.\",\n",
        "    \"Roger Dodger is one of the least compelling variations on this theme.\",\n",
        "    \"Roger Dodger is at least compelling as a variation on the theme.\",\n",
        "    \"They fall in love with the product.\",\n",
        "    \"But then it breaks.\",\n",
        "    \"Usually around the time the 90-day warranty expires.\",\n",
        "    \"The twin towers collapsed today.\",\n",
        "    \"However, Mr. Carter solemnly argues, his client carried out the kidnapping under orders and in the 'least offensive way possible.'\"\n",
        "]\n",
        "\n",
        "# Add tricky sentences to the list\n",
        "sentences.extend(tricky_sentences)\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer for VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Analyze the sentiment of each sentence and print the results\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    ss = sid.polarity_scores(sentence)\n",
        "    for k in sorted(ss):\n",
        "        print(f'{k}: {ss[k]}, ', end='')\n",
        "    print()  # Add newline after each sentence's sentiment analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee_PMgy1WoDC",
        "outputId": "376eb45c-7e0c-41e2-d583-21c02a942131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VADER is smart, handsome, and funny.\n",
            "compound: 0.8316, neg: 0.0, neu: 0.254, pos: 0.746, \n",
            "VADER is smart, handsome, and funny!\n",
            "compound: 0.8439, neg: 0.0, neu: 0.248, pos: 0.752, \n",
            "VADER is very smart, handsome, and funny.\n",
            "compound: 0.8545, neg: 0.0, neu: 0.299, pos: 0.701, \n",
            "VADER is VERY SMART, handsome, and FUNNY.\n",
            "compound: 0.9227, neg: 0.0, neu: 0.246, pos: 0.754, \n",
            "VADER is VERY SMART, handsome, and FUNNY!!!\n",
            "compound: 0.9342, neg: 0.0, neu: 0.233, pos: 0.767, \n",
            "VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\n",
            "compound: 0.9469, neg: 0.0, neu: 0.294, pos: 0.706, \n",
            "The book was good.\n",
            "compound: 0.4404, neg: 0.0, neu: 0.508, pos: 0.492, \n",
            "The book was kind of good.\n",
            "compound: 0.3832, neg: 0.0, neu: 0.657, pos: 0.343, \n",
            "The plot was good, but the characters are uncompelling.\n",
            "compound: -0.1027, neg: 0.208, neu: 0.619, pos: 0.173, \n",
            "A really bad, horrible book.\n",
            "compound: -0.8211, neg: 0.791, neu: 0.209, pos: 0.0, \n",
            ":) and :D\n",
            "compound: 0.7925, neg: 0.0, neu: 0.124, pos: 0.876, \n",
            "\n",
            "compound: 0.0, neg: 0.0, neu: 0.0, pos: 0.0, \n",
            "Today sux\n",
            "compound: -0.3612, neg: 0.714, neu: 0.286, pos: 0.0, \n",
            "Today sux!\n",
            "compound: -0.4199, neg: 0.736, neu: 0.264, pos: 0.0, \n",
            "Today SUX!\n",
            "compound: -0.5461, neg: 0.779, neu: 0.221, pos: 0.0, \n",
            "Today kinda sux! But I'll get by, lol\n",
            "compound: 0.5249, neg: 0.138, neu: 0.517, pos: 0.344, \n",
            "\n",
            "It was one of the worst movies I've seen, despite good reviews.\n",
            "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
            "Unbelievably bad acting!!\n",
            "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
            "Poor direction.\n",
            "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
            "VERY poor production.\n",
            "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
            "The movie was bad.\n",
            "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
            "Very bad movie.\n",
            "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
            "VERY bad movie.\n",
            "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
            "VERY BAD movie.\n",
            "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
            "VERY BAD movie!\n",
            "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n",
            "Most automated sentiment analysis tools are shit.\n",
            "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, \n",
            "VADER sentiment analysis is the shit.\n",
            "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, \n",
            "Sentiment analysis has never been good.\n",
            "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, \n",
            "Sentiment analysis with VADER has never been this good.\n",
            "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, \n",
            "Warren Beatty has never been so entertaining.\n",
            "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, \n",
            "I won't say that the movie is astounding and I wouldn't claim that the movie is too banal either.\n",
            "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, \n",
            "I like to hate Michael Bay films, but I couldn't fault this one.\n",
            "compound: 0.3153, neg: 0.157, neu: 0.534, pos: 0.309, \n",
            "I like to hate Michael Bay films, BUT I couldn't help but fault this one.\n",
            "compound: -0.1531, neg: 0.277, neu: 0.477, pos: 0.246, \n",
            "It's one thing to watch an Uwe Boll film, but another thing entirely to pay for it.\n",
            "compound: -0.2541, neg: 0.112, neu: 0.888, pos: 0.0, \n",
            "The movie was too good.\n",
            "compound: 0.4404, neg: 0.0, neu: 0.58, pos: 0.42, \n",
            "This movie was actually neither that funny, nor super witty.\n",
            "compound: -0.6759, neg: 0.41, neu: 0.59, pos: 0.0, \n",
            "This movie doesn't care about cleverness, wit or any other kind of intelligent humor.\n",
            "compound: -0.1338, neg: 0.265, neu: 0.497, pos: 0.239, \n",
            "Those who find ugly meanings in beautiful things are corrupt without being charming.\n",
            "compound: -0.3553, neg: 0.314, neu: 0.493, pos: 0.192, \n",
            "There are slow and repetitive parts, BUT it has just enough spice to keep it interesting.\n",
            "compound: 0.4678, neg: 0.079, neu: 0.735, pos: 0.186, \n",
            "The script is not fantastic, but the acting is decent and the cinematography is EXCELLENT!\n",
            "compound: 0.7565, neg: 0.092, neu: 0.607, pos: 0.301, \n",
            "Roger Dodger is one of the most compelling variations on this theme.\n",
            "compound: 0.2944, neg: 0.0, neu: 0.834, pos: 0.166, \n",
            "Roger Dodger is one of the least compelling variations on this theme.\n",
            "compound: -0.1695, neg: 0.132, neu: 0.868, pos: 0.0, \n",
            "Roger Dodger is at least compelling as a variation on the theme.\n",
            "compound: 0.2263, neg: 0.0, neu: 0.84, pos: 0.16, \n",
            "They fall in love with the product.\n",
            "compound: 0.6369, neg: 0.0, neu: 0.588, pos: 0.412, \n",
            "But then it breaks.\n",
            "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
            "Usually around the time the 90-day warranty expires.\n",
            "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
            "The twin towers collapsed today.\n",
            "compound: -0.2732, neg: 0.344, neu: 0.656, pos: 0.0, \n",
            "However, Mr. Carter solemnly argues, his client carried out the kidnapping under orders and in the 'least offensive way possible.'\n",
            "compound: 0.1729, neg: 0.109, neu: 0.712, pos: 0.179, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "# text = \"I absolutely love this product! It exceeded my expectations.\\nI am not satisfied with the service.\\nOverall, it was an okay experience.\"\n",
        "text = text\n",
        "\n",
        "# Split the text by newline characters\n",
        "lines = text.split('\\n')\n",
        "\n",
        "# Iterate over each line and analyze sentiment\n",
        "for line in lines:\n",
        "    if line.strip():  # Check if the line is not empty\n",
        "        # Use spaCy to process the text (optional, for demonstration purposes)\n",
        "        doc = nlp(line)\n",
        "\n",
        "        # Perform sentiment analysis using TextBlob\n",
        "        blob = TextBlob(line)\n",
        "        sentiment_polarity = blob.sentiment.polarity\n",
        "\n",
        "        # Interpret sentiment polarity\n",
        "        if sentiment_polarity > 0:\n",
        "            sentiment = \"Positive\"\n",
        "        elif sentiment_polarity < 0:\n",
        "            sentiment = \"Negative\"\n",
        "        else:\n",
        "            sentiment = \"Neutral\"\n",
        "\n",
        "        print(\"Text to analyze: {0}\\n\".format(line))\n",
        "        print(f\"The sentiment of the text is: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhFz1bl7MYCI",
        "outputId": "839b4679-ad16-4682-accf-3e2c25bce76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text to analyze: Tesla opened a new office in Tokyo today\n",
            "\n",
            "The sentiment of the text is: Positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mediapipe\n",
        "\n",
        "Also [Try Mediapipe approach](https://mediapipe-studio.webapps.google.com/demo/text_classifier)\n",
        "\n"
      ],
      "metadata": {
        "id": "mjmvT84yOS00"
      }
    }
  ]
}